{
 "cells": [
  {
   "cell_type": "raw",
   "id": "56320947-88ee-4854-b778-ce3fd9d90483",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e953a13-56c3-4d4e-837b-67f5352361df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries for tokenizing by sentence and words\n",
    "import nltk\n",
    "# Installing NLTK data for the first time\n",
    "# nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece7f3fa-a498-4952-80f5-dfe41ace3b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String for tokenizing\n",
    ">>> example_string = \"\"\"\n",
    "Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "And the first lesson of all was the basic trust that he could learn.\n",
    "It's shocking to find how many people do not believe they can learn,\n",
    "and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dea141-c482-4163-a81f-0c56c6360db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting up into sentences i.e. Tokenizing by sentences\n",
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234ea64e-59df-432e-90a1-0dba340e76a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting up into words i.e. Tokenizing by words\n",
    "li = word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cec3951e-6e0b-45d4-9920-df1e6e0cef1c",
   "metadata": {},
   "source": [
    "Filtering Stop Words: \n",
    "   - Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7fda69-0fca-4693-995b-a20fd06363e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1791dc2-5fe6-40df-9ccd-a87f2248d182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d0f615-7f12-4f7a-9442-10755049088d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_quote = word_tokenize(worf_quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c1c140-09b8-4a4a-b659-c6b563970571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) # Creating a list of all stopwords\n",
    "filtered_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc46c1c-281c-4d99-8ac6-a3dacc4233b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We are filtering out all the tokenized words that are considered as stopwords\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "         filtered_list.append(word)\n",
    "            \n",
    "''' We are filtering out all the tokenized words that are considered as stopwords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64503478-f833-4420-bc2c-9a3a2e211608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sir', ',', 'protest', '.', 'merry', 'man', '!']\n"
     ]
    }
   ],
   "source": [
    "''' We can see that some of the stopwords have been removed'''\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47df5b3-1da8-4876-b224-4c40a17bd909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' OR '''\n",
    "filtered_list = [\n",
    "     word for word in words_in_quote if word.casefold() not in stop_words\n",
    " ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3e8ebb8-888a-461c-9277-2599ff051cc5",
   "metadata": {},
   "source": [
    "Stemming:\n",
    "    - a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and \n",
    "    “helper” share the root “help.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1d1cd2-ac1d-4e1f-b0c1-6ea1b6a89252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86830580-e178-4980-a360-c2ac1c042386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a stemmer out of PortStemmer\n",
    "Stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ffe650-6d77-463d-b2a6-f38b26397b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "    The crew of the USS Discovery discovered many discoveries.\n",
    "    Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f46be7-ef10-42ec-98d7-79307fc5da55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discoveries',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorers',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_stemming)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4229e2-2140-40af-8da7-299f9a744f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [ Stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f271ec63-12f2-4215-8493-009b735abb74",
   "metadata": {},
   "source": [
    "\n",
    "Original word   \t    Stemmed version\n",
    "'Discovery'     \t'   discoveri'\n",
    "'discovered'    \t'   discov'\n",
    "'discoveries'   \t'   discoveri'\n",
    "'Discovering'   \t'   discov'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c140358b-407c-467d-92a7-f4cc17c7cba6",
   "metadata": {},
   "source": [
    "Those results look a little inconsistent. Why would 'Discovery' give you 'discoveri' when 'Discovering' gives you 'discov'?\n",
    "\n",
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "    - Understemming happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "    - Overstemming happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5ae0bd3-2f8d-41ac-ab4f-6238af776c7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tagging Parts of Speech:\n",
    "    - a grammatical term that deals with the roles words play when you use them together in sentences.\n",
    "      i.e.  the task of labeling the words in your text according to their part of speech."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c92c12a2-9b0b-40eb-a346-46a72ab75f6b",
   "metadata": {},
   "source": [
    "Part of speech  \t                Role                                                            \t          Examples\n",
    "Noun            \t     Is a person, place, or thing                                               \t   mountain, bagel, Poland\n",
    "Pronoun         \t     Replaces a noun                                                            \t   you, she, we\n",
    "Adjective       \t     Gives information about what a noun is like                                \t   efficient, windy, colorful\n",
    "Verb            \t     Is an action or a state of being                                           \t   learn, is, go\n",
    "Adverb          \t     Gives information about a verb, an adjective, or another adverb            \t   efficiently, always, very\n",
    "Preposition     \t     Gives information about how a noun or pronoun is connected to another word \t   from, about, at\n",
    "Conjunction     \t     Connects two other words or phrases                                        \t   so, because, and\n",
    "Interjection    \t     Is an exclamation                                                          \t   yay, ow, wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638c94c5-cfbc-4b94-864a-6b498ead6a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a text to tag\n",
    "sagan_quote = \"\"\"\n",
    "    If you wish to make an apple pie from scratch,\n",
    "    you must first invent the universe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5e25a5-30f2-4984-86cc-b018475bcc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'make',\n",
       " 'an',\n",
       " 'apple',\n",
       " 'pie',\n",
       " 'from',\n",
       " 'scratch',\n",
       " ',',\n",
       " 'you',\n",
       " 'must',\n",
       " 'first',\n",
       " 'invent',\n",
       " 'the',\n",
       " 'universe',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_sagan_quote = word_tokenize(sagan_quote)\n",
    "words_in_sagan_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c449a2-ba14-43a0-8682-aee741afda25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apple', 'NN'),\n",
       " ('pie', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('scratch', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('first', 'VB'),\n",
       " ('invent', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging Parts of speech on the tokienized words\n",
    "nltk.pos_tag(words_in_sagan_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d56f936-1f93-44e7-8d10-e1cb58beacb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset() # i.e. getting the list of tags and their meanings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be996319-bd6e-4238-831a-7b82f3b08d30",
   "metadata": {},
   "source": [
    "- Lemmatization and Stemming, both are used to generate root form of derived (inflected) words. However, lemma is an actual language word, \n",
    "  whereas stem may not be an actual word.\n",
    "\n",
    "- Lemmatization uses corpus for stop words and WordNet corpus to produce lemma. Moreover, parts-of-speech also had to be defined to obtain correct lemma.\n",
    "\n",
    "- So, how to decide when to use what! If speed is important, use stemming as lemmatization scan the entire corpus which is a time-consuming task. \n",
    "  Secondly, whether stemmers or lemmatizers should be used depends on the application we are working. Finally, if language is important while building a \n",
    "  language application, lemmatization is used which scans a corpus to match root forms."
   ]
  },
  {
   "cell_type": "raw",
   "id": "43ce2add-d64a-41f5-a3ac-f2d4bcd9b521",
   "metadata": {},
   "source": [
    "Lemmatizing:\n",
    "    - reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ab1f78-71d2-4f22-a97c-c4d61760a549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing WordLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Creating the lemmatizer to lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "133ab2d8-9317-46db-9698-b657e55a09ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String for Lemmatizing\n",
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3d6cd9-ec0c-470a-94e1-703740a84f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aeeb6b1-e20a-46e3-ad29-8b9730b47acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a612be5b-a953-48fa-be3e-a8a108422fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('worst',pos='a')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd70b4fe-eee6-4d1a-bc29-41125e335f46",
   "metadata": {},
   "source": [
    "Chunking\n",
    "While tokenizing allows you to identify words and sentences, chunking allows you to identify phrases.\n",
    "\n",
    "Here are some examples:\n",
    "“A planet”\n",
    "“A tilting planet”\n",
    "“A swiftly tilting planet”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a53faf34-bd0d-451f-b823-ba359e7696ae",
   "metadata": {},
   "source": [
    "Steps involved in Chunking:\n",
    "- Tokenize the words\n",
    "- Tag parts of speech\n",
    "- Create a chunk grammar with one regular expression rule\n",
    "- create chunk parser with this grammar rule\n",
    "- use the parser\n",
    "\n",
    "Note: \n",
    "- A chunk grammar is a combination of rules on how sentences should be chunked. It often uses regular expressions, or regexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "489260c6-7ec1-4d7d-bc27-be9b8c7c1cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0547e09-900c-427f-ac61-7b592a3f2e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('dangerous', 'JJ'),\n",
       " ('business', 'NN'),\n",
       " (',', ','),\n",
       " ('Frodo', 'NNP'),\n",
       " (',', ','),\n",
       " ('going', 'VBG'),\n",
       " ('out', 'RP'),\n",
       " ('your', 'PRP$'),\n",
       " ('door', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_lotr = word_tokenize(lotr_quote)\n",
    "lotr_pos_tag = nltk.pos_tag(words_in_lotr)\n",
    "lotr_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d911aed3-b6c0-419c-b8ae-c8c3c3e14e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a chunk grammer with one regular expression rule\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "'''\n",
    "According to the rule you created, your chunks:\n",
    "    - Start with an optional (?) determiner ('DT')\n",
    "    - Can have any number (*) of adjectives (JJ)\n",
    "    - End with a noun (<NN>)'''\n",
    "\n",
    "# Creating a chunk parser with this grammer rule\n",
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e13f3c5-abfc-40fb-b015-ac1d42803100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying the parser with the our quote\n",
    "tree = chunk_parser.parse(lotr_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "448540e3-a40b-4a48-a05c-0351e6eb7fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46114d1a-8635-4043-b8ab-d8ba47aa0509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "271ac3f6-ea09-4b4d-aa63-c1cd063157f8",
   "metadata": {},
   "source": [
    "Chinking\n",
    "Chinking is used together with chunking, but while chunking is used to include a pattern, chinking is used to exclude a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddedfcc3-85ee-478f-bbf3-6f80aad2c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    " Chunk: {<.*>+}\n",
    "        }<JJ>{\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c0d24a-a950-4b0a-a81e-ed683cc9b7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57c1998b-0155-4624-afd8-0b60c1194077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = chunk_parser.parse(lotr_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73633077-a14d-486d-9bb6-621fca3bdd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.draw of Tree('S', [Tree('Chunk', [('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT')]), ('dangerous', 'JJ'), Tree('Chunk', [('business', 'NN'), (',', ','), ('Frodo', 'NNP'), (',', ','), ('going', 'VBG'), ('out', 'RP'), ('your', 'PRP$'), ('door', 'NN'), ('.', '.')])])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6976c7a-52b8-4354-b102-24fbc9edad4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4a9ff-08f2-46d8-8cbd-ec30793de9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
