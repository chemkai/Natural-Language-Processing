{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5283d93d-5d5a-4701-9777-4737422e599e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/90/f0/0133b684e18932c7bf4075d94819746cee2c0329f2569db526b0fa1df1df/spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/74/24/564a7df5b1fac0520f6b55137deea2cc0b6f7d6e66228f1645dbfd59bb33/thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     ------------------- ------------------- 92.2/181.6 kB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 163.8/181.6 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 181.6/181.6 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kaimo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB 8.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.4/12.1 MB 5.5 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.6/12.1 MB 4.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 4.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.9/12.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.7/12.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/12.1 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/12.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.1/12.1 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.4/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.8/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.2/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.9/12.1 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB ? eta 0:00:00\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 368.6/479.7 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 479.7/479.7 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 15.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.4/6.6 MB 11.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.6 MB 7.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.6 MB 7.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.2/6.6 MB 4.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.4/6.6 MB 4.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.4/6.6 MB 4.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.6 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.2/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.0/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.8/6.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.8/6.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.1/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.4/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.9/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, typer, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa86191-fe14-4b80-9793-6699131aa13d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4014ad1a-4e4d-4109-abf6-e5db080730f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8369ecc-eeeb-4789-abf4-8e2969095264",
   "metadata": {},
   "source": [
    "Creation of the Doc object in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d539158-ca45-4716-afb7-d415471f9e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1bbeb248cd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the language model instance in spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d32bca95-7700-4921-8034-3d1f7d9c8494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing a Doc Object with String as an input argument\n",
    "introduction_doc = nlp(' This is a tutorial about NLP in spacy')\n",
    "type(introduction_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7b8a841-1175-4c5a-9750-d8045187c085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'This', 'is', 'a', 'tutorial', 'about', 'NLP', 'in', 'spacy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Doc object is a sequence of token objects,\n",
    "- Each token object has an innformation about a particular piece '''\n",
    "[token.text for token in introduction_doc] # On each token object, we called .text attribute to get the text contained within that object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9184bba8-5a66-4a5a-86e2-64cb3eeafd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "file_name = 'NLP.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59913c89-7501-45f6-aaa4-2cb234af1565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'spacy']\n"
     ]
    }
   ],
   "source": [
    "# Reading from the file\n",
    "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding='utf-8'))\n",
    "print([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "074cf48e-e85e-42eb-8b15-11118af4507f",
   "metadata": {},
   "source": [
    "Sentence Detection:\n",
    "    - Process of locating where sentences starts and end in a given text.\n",
    "    - Allows text to be divided into lingustically meaningful units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbb3cb86-4cf5-48e9-b992-0fad62620932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_text = '''\n",
    "Chem is an aspiring ML developer. He wants to master NLP and computer vision. He looks forward to work as a remote developer for some foreign company someday'''\n",
    "\n",
    "about_doc = nlp(about_text) # Creating a Doc object of the above string\n",
    "sentences = list(about_doc.sents) # .sents property is used to extract sentences from the doc object created above\n",
    "len(sentences) # checking the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a50397-7b0e-4d6e-8a58-1ac238b5d609",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chem is an aspiring...\n",
      "He wants to master NLP...\n",
      "He looks forward to work...\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"{sentence[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bfdff98-9766-4eed-895a-c8b5674c3052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Customization of sentence detection behaviour can also be done by using custom delimiters '''\n",
    "\n",
    "ellipsis_text = (\n",
    "     \"Gus, can you, ... never mind, I forgot\"\n",
    "     \" what I was saying. So, do you think\"\n",
    "     \" we should ...\"\n",
    ")\n",
    "from spacy.language import Language\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "# used the @Language.component(\"set_custom_boundaries\") decorator to define a new function that takes a Doc object as an argument\n",
    "\n",
    "def set_custom_boundaries(doc):\n",
    "    \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4849243-724d-4d0d-890b-9c73100abe8f",
   "metadata": {},
   "source": [
    "Tokens in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "407c7f1f-e389-4851-952c-537d84996a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chem 0\n",
      "is 5\n",
      "an 8\n",
      "aspiring 11\n",
      "Ml 20\n",
      "engineer 23\n",
      "currently 32\n",
      "learning 42\n",
      "Ml 51\n",
      "and 54\n",
      "NLP 58\n",
      ". 61\n",
      "He 63\n",
      "is 66\n",
      "very 69\n",
      "much 74\n",
      "interested 79\n",
      "in 90\n",
      "learning 93\n",
      "NLP 102\n",
      "and 106\n",
      "computer 110\n",
      "vision 119\n",
      ". 125\n"
     ]
    }
   ],
   "source": [
    "about_text = (\"Chem is an aspiring Ml engineer currently learning Ml and NLP. He is very much interested in learning NLP and computer vision.\")\n",
    "about_doc = nlp(about_text)\n",
    "for token in about_doc:\n",
    "    print(token,token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec0e182f-b986-44cd-a47c-b83500999635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
      "Chem                  True           False             False\n",
      "is                    True           False             True\n",
      "an                    True           False             True\n",
      "aspiring              True           False             False\n",
      "Ml                    True           False             False\n",
      "engineer              True           False             False\n",
      "currently             True           False             False\n",
      "learning              True           False             False\n",
      "Ml                    True           False             False\n",
      "and                   True           False             True\n",
      "NLP                   True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "very                  True           False             True\n",
      "much                  True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "NLP                   True           False             False\n",
      "and                   True           False             True\n",
      "computer              True           False             False\n",
      "vision                True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'Text with Whitespace':22}\"\n",
    "    f\"{'Is Alphanumeric?':15}\"\n",
    "    f\"{'Is Punctuation?':18}\"\n",
    "    f\"{'Is Stop Word?'}\"\n",
    ")\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"{str(token.text_with_ws):22}\"     \n",
    "        f\"{str(token.is_alpha):15}\"\n",
    "        f\"{str(token.is_punct):18}\"\n",
    "        f\"{str(token.is_stop)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0015bff-f136-4634-b747-5f168d399c2a",
   "metadata": {},
   "source": [
    "Stop Words:\n",
    "    - Most common words in a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f03dcf7b-629f-4448-afeb-7f535dffc221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c944005-4ac0-44ae-bbbc-601f2c81587f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still\n",
      "sometimes\n",
      "show\n",
      "done\n",
      "from\n",
      "ourselves\n",
      "anything\n",
      "that\n",
      "herself\n",
      "fifty\n"
     ]
    }
   ],
   "source": [
    "for stopword in list(spacy_stopwords)[:10]:\n",
    "    print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9db43be-6d73-4835-be78-69aa25e06009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ , Chem, learning, ML, ., Chem, believes, better, 1, %, everyday, ., learns, resources, codebasics, ,, RealPython, \n",
      ", articles, .]\n"
     ]
    }
   ],
   "source": [
    "custom_about_text = ''' Chem is learning ML. Chem believes that he can get better 1% everyday. He learns from various resources such as codebasics, RealPython and\n",
    "other articles.'''\n",
    "about_doc = nlp(custom_about_text)\n",
    "print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31946209-5b19-4eb3-9504-705001059915",
   "metadata": {},
   "source": [
    "Lemmatization:\n",
    "    - process of reducing inflicted forms of word while still ensuring that the reduced form belongs to the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "352c8788-8e46-42e7-9ce5-0471935364a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is : be\n",
      "                  He : he\n",
      "               keeps : keep\n",
      "          organizing : organize\n",
      "             meetups : meetup\n",
      "               talks : talk\n"
     ]
    }
   ],
   "source": [
    "conference_help_text = (\"Gus is helping organize a developer\"\n",
    "     \" conference on Applications of Natural Language\"\n",
    "     \" Processing. He keeps organizing local Python meetups\"\n",
    "     \" and several internal talks at his workplace.\")\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c35ff437-e954-401f-8a15-73d787fdb252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "complete_text = (\n",
    "     \"Gus Proto is a Python developer currently\"\n",
    "     \" working for a London-based Fintech company. He is\"\n",
    "     \" interested in learning Natural Language Processing.\"\n",
    "     \" There is a developer conference happening on 21 July\"\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "     ' Language Processing\". There is a helpline number'\n",
    "     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "     \" He keeps organizing local Python meetups and several\"\n",
    "     \" internal talks at his workplace. Gus is also presenting\"\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "     ' cases of Natural Language Processing in Fintech\".'\n",
    "     \" Apart from his work, he is very passionate about music.\"\n",
    "     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "     \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)\n",
    "words = [token.text for token in complete_doc if not token.is_stop and not token.is_punct]\n",
    "print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a890393e-cad8-441a-ba4e-0d71aabbf264",
   "metadata": {},
   "source": [
    "Part of Speech Tagging:\n",
    "    - part of speech is a grammatical role that explains how a particular word is used in a sentence.\n",
    "    - There are typically eight Parts of speech.\n",
    "    1. Noun\n",
    "    2. Pronoun\n",
    "    3. Adjective\n",
    "    4. Verb\n",
    "    5. Adverb\n",
    "    6. Preposition\n",
    "    7. Conjuction\n",
    "    8. Interjection\n",
    "    - i.e.Part-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfc705b0-421a-477e-a1b5-a6e62f7aa886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN:  \n",
      "=====\n",
      "AG: _SP        POS: SPACE\n",
      "EXPLANATION: whitespace\n",
      "\n",
      "TOKEN: Chem\n",
      "=====\n",
      "AG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "AG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "AG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: ML\n",
      "=====\n",
      "AG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "AG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: Chem\n",
      "=====\n",
      "AG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: believes\n",
      "=====\n",
      "AG: VBZ        POS: VERB\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: that\n",
      "=====\n",
      "AG: IN         POS: SCONJ\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: he\n",
      "=====\n",
      "AG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "AG: MD         POS: AUX\n",
      "EXPLANATION: verb, modal auxiliary\n",
      "\n",
      "TOKEN: get\n",
      "=====\n",
      "AG: VB         POS: VERB\n",
      "EXPLANATION: verb, base form\n",
      "\n",
      "TOKEN: better\n",
      "=====\n",
      "AG: JJR        POS: ADJ\n",
      "EXPLANATION: adjective, comparative\n",
      "\n",
      "TOKEN: 1\n",
      "=====\n",
      "AG: CD         POS: NUM\n",
      "EXPLANATION: cardinal number\n",
      "\n",
      "TOKEN: %\n",
      "=====\n",
      "AG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: everyday\n",
      "=====\n",
      "AG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "AG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "=====\n",
      "AG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: learns\n",
      "=====\n",
      "AG: VBZ        POS: VERB\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: from\n",
      "=====\n",
      "AG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: various\n",
      "=====\n",
      "AG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: resources\n",
      "=====\n",
      "AG: NNS        POS: NOUN\n",
      "EXPLANATION: noun, plural\n",
      "\n",
      "TOKEN: such\n",
      "=====\n",
      "AG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: as\n",
      "=====\n",
      "AG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: codebasics\n",
      "=====\n",
      "AG: NNS        POS: NOUN\n",
      "EXPLANATION: noun, plural\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "AG: ,          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, comma\n",
      "\n",
      "TOKEN: RealPython\n",
      "=====\n",
      "AG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "AG: CC         POS: CCONJ\n",
      "EXPLANATION: conjunction, coordinating\n",
      "\n",
      "TOKEN: \n",
      "\n",
      "=====\n",
      "AG: _SP        POS: SPACE\n",
      "EXPLANATION: whitespace\n",
      "\n",
      "TOKEN: other\n",
      "=====\n",
      "AG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: articles\n",
      "=====\n",
      "AG: NNS        POS: NOUN\n",
      "EXPLANATION: noun, plural\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "AG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     print(f\"\"\"TOKEN: {str(token)}\\n=====\\nAG: {str(token.tag_):10} POS: {token.pos_}\\nEXPLANATION: {spacy.explain(token.tag_)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11469de0-9a8f-4e21-8393-aa1c312324ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gus',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'developer',\n",
       " 'currently',\n",
       " 'work',\n",
       " 'london',\n",
       " 'base',\n",
       " 'fintech',\n",
       " 'company',\n",
       " 'interested',\n",
       " 'learn',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'developer',\n",
       " 'conference',\n",
       " 'happen',\n",
       " '21',\n",
       " 'july',\n",
       " '2019',\n",
       " 'london',\n",
       " 'title',\n",
       " 'application',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'helpline',\n",
       " 'number',\n",
       " 'available',\n",
       " '+44',\n",
       " '1234567891',\n",
       " 'gus',\n",
       " 'helping',\n",
       " 'organize',\n",
       " 'keep',\n",
       " 'organize',\n",
       " 'local',\n",
       " 'python',\n",
       " 'meetup',\n",
       " 'internal',\n",
       " 'talk',\n",
       " 'workplace',\n",
       " 'gus',\n",
       " 'present',\n",
       " 'talk',\n",
       " 'talk',\n",
       " 'introduce',\n",
       " 'reader',\n",
       " 'use',\n",
       " 'case',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'fintech',\n",
       " 'apart',\n",
       " 'work',\n",
       " 'passionate',\n",
       " 'music',\n",
       " 'gus',\n",
       " 'learn',\n",
       " 'play',\n",
       " 'piano',\n",
       " 'enrol',\n",
       " 'weekend',\n",
       " 'batch',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'great',\n",
       " 'piano',\n",
       " 'academy',\n",
       " 'situate',\n",
       " 'mayfair',\n",
       " 'city',\n",
       " 'london',\n",
       " 'world',\n",
       " 'class',\n",
       " 'piano',\n",
       " 'instructor']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text = (\n",
    "     \"Gus Proto is a Python developer currently\"\n",
    "     \" working for a London-based Fintech company. He is\"\n",
    "     \" interested in learning Natural Language Processing.\"\n",
    "     \" There is a developer conference happening on 21 July\"\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "     ' Language Processing\". There is a helpline number'\n",
    "     \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "     \" He keeps organizing local Python meetups and several\"\n",
    "     \" internal talks at his workplace. Gus is also presenting\"\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "     ' cases of Natural Language Processing in Fintech\".'\n",
    "     \" Apart from his work, he is very passionate about music.\"\n",
    "     \" Gus is learning to play the Piano. He has enrolled\"\n",
    "     \" himself in the weekend batch of Great Piano Academy.\"\n",
    "     \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "     \" of London and has world-class piano instructors.\")\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "def is_token_allowed(token):\n",
    "     return bool(token and str(token).strip() and not token.is_stop and not token.is_punct)\n",
    "    \n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [preprocess_token(token) for token in complete_doc if is_token_allowed(token)]\n",
    "\n",
    "complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e243-d26d-41f3-9d58-370bf3f0ddce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
